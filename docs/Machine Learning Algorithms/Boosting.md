---
id: Machine Learning Algorithms - Boosting
title: Boosting
sidebar_position: 3
---

- The boosting paradigm allows the learner to have smooth control over this tradeoff. The learning starts with a basic class (that might have a large approximation error), and as it progresses the class that the predictor may belong to grows richer.
- A boosting algorithm amplifies the accuracy of *weak learners*. When a weak learner can be implemented efficiently, boosting provides a tool for aggregating such weak hypotheses to approximate gradually good predictors for larger, and harder to learn, classes.

### Weak Learnability

- A learning algorithm, $A$, is a $\gamma$-weak-learner for a class $\mathcal{H}$ if there exists a function $m_{\mathcal{H}}:(0,1) \rightarrow \mathbb{N}$ such that for every $\delta \in(0,1)$, for every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labeling function $f: \mathcal{X} \rightarrow\{\pm 1\}$, if the realizable assumption holds with respect to $\mathcal{H}, \mathcal{D}, f$, then when running the learning algorithm on $m \geq m_{\mathcal{H}}(\delta)$ i.i.d. examples generated by $\mathcal{D}$ and labeled by $f$, the algorithm returns a hypothesis $h$ such that, with probability of at least $1-\delta, L_{(\mathcal{D}, f)}(h) \leq 1 / 2-\gamma$.
- A hypothesis class $\mathcal{H}$ is $\gamma$-weak-learnable if there exists a $\gamma$-weak-learner for that class.