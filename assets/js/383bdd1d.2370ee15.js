"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7444],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>b});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),c=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(a),p=r,b=u["".concat(s,".").concat(p)]||u[p]||m[p]||o;return a?n.createElement(b,i(i({ref:t},d),{},{components:a})):n.createElement(b,i({ref:t},d))}));function b(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},83824:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const o={id:"Code - Parallel_Computation - Labs - Lab 5",title:"Lab 5",sidebar_position:5},i=void 0,l={unversionedId:"Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 5",id:"Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 5",title:"Lab 5",description:"---",source:"@site/docs/Code/Parallel_Computation/Labs/Lab5.md",sourceDirName:"Code/Parallel_Computation/Labs",slug:"/Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 5",permalink:"/myWiki/Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 5",draft:!1,editUrl:"https://github.com/pwang649/myWiki/edit/main/docs/Code/Parallel_Computation/Labs/Lab5.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{id:"Code - Parallel_Computation - Labs - Lab 5",title:"Lab 5",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Lab 4",permalink:"/myWiki/Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 4"},next:{title:"Lab 6",permalink:"/myWiki/Code/Parallel_Computation/Labs/Code - Parallel_Computation - Labs - Lab 6"}},s={},c=[{value:"Naive MM on CUDA",id:"naive-mm-on-cuda",level:3},{value:"Kernal Function",id:"kernal-function",level:4},{value:"Result",id:"result",level:4},{value:"Block MM on CUDA",id:"block-mm-on-cuda",level:3},{value:"Kernal Function",id:"kernal-function-1",level:4},{value:"Result",id:"result-1",level:4}],d={toc:c},u="wrapper";function m(e){let{components:t,...a}=e;return(0,r.kt)(u,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("hr",null),(0,r.kt)("h3",{id:"naive-mm-on-cuda"},"Naive MM on CUDA"),(0,r.kt)("h4",{id:"kernal-function"},"Kernal Function"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-c"},"__global__ void matrix_multiplication(int *a, int *b, int *c)\n{\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    for (int i = 0; i < size; i++)\n        c[row * size + col] += a[row * size + i] * b[i * size + col];\n}\n")),(0,r.kt)("h4",{id:"result"},"Result"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"time is 8847619.000000 ns\nc[451][451]=2048\n")),(0,r.kt)("h3",{id:"block-mm-on-cuda"},"Block MM on CUDA"),(0,r.kt)("h4",{id:"kernal-function-1"},"Kernal Function"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-c"},"__global__ void matrix_multiplication(int *a, int *b, int *c)\n{\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    int threadX = threadIdx.x;\n    int threadY = threadIdx.y;\n\n    int i, j;\n    int result = 0;\n    __shared__ int a_s[32][32];\n    __shared__ int b_s[32][32];\n\n    for (i = 0; i < 32; i++)\n    {\n        a_s[threadX][threadY] = a[row * size + (i * 32 + threadY)];\n        b_s[threadX][threadY] = b[(i * 32 + threadX) * size + col];\n        __syncthreads();\n        for (j = 0; j < 32; j++)\n        {\n            result += a_s[threadX][j] * b_s[j][threadY];\n        }\n        __syncthreads();\n    }\n    c[row * size + col] = result;\n}\n")),(0,r.kt)("h4",{id:"result-1"},"Result"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"time is 7144499.000000 ns\nc[451][451]=2048\n")),(0,r.kt)("admonition",{title:"Observation",type:"info"},(0,r.kt)("p",{parentName:"admonition"},"The second method is faster than the first. The reason is that fetching data from the global memory of the GPU takes more time than the shared memory in the SM. The second method allocates data to the shared memory first, then operate back and forth whereas the first one waste time transferring data via the global memory.")))}m.isMDXComponent=!0}}]);